{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     10\u001b[0m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmixed_precision\u001b[38;5;241m.\u001b[39mset_global_policy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed_float16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import h5py\n",
    "import netCDF4\n",
    "import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "print(\"GPUs available: \", tf.config.list_physical_devices('GPU'))\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(physical_devices)\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "#\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"SIC_Attention_Res_UNet\"\n",
    "lead_time = 0\n",
    "#\n",
    "function_path = \"/lustre/storeB/users/cyrilp/COSI/Scripts/Operational/\" + experiment_name + \"/\"\n",
    "sys.path.insert(0, function_path)\n",
    "from Data_generator_UNet import *\n",
    "from Attention_Res_UNet import *\n",
    "#\n",
    "date_min_test = \"20220101\"\n",
    "date_max_test = \"20221231\"\n",
    "#\n",
    "paths = {}\n",
    "paths[\"training\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Training/\"\n",
    "paths[\"standard\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Standardization/\"\n",
    "paths[\"model_weights\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Model_weights/\" + experiment_name + \"/\"\n",
    "paths[\"ice_edge_lengths\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/AMSR2_ice_edge_lengths/\"\n",
    "paths[\"predictions_netCDF\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Predictions/\" + experiment_name + \"/lead_time_\" + str(lead_time) + \"_days/netCDF/\"\n",
    "paths[\"prediction_scores\"] = \"/lustre/storeB/project/copernicus/cosi/WP3/Operational/Predictions/\" + experiment_name + \"/lead_time_\" + str(lead_time) + \"_days/scores/\"\n",
    "#\n",
    "for var in paths:\n",
    "    if os.path.isdir(paths[var]) == False:\n",
    "        os.system(\"mkdir -p \" + paths[var])\n",
    "#\n",
    "file_standardization = paths[\"standard\"] + \"Stats_standardization_20130103_20201231_weekly.h5\"\n",
    "file_model_weights = paths[\"model_weights\"] + \"UNet_leadtime_\" + str(lead_time) + \"_days.h5\"\n",
    "#\n",
    "grid_cell_area = 5000 ** 2  # m2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# U-Net parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_predictors = [\"LSM\", \"ECMWF_T2M_cum\", \"ECMWF_wind_x_cum\", \"ECMWF_wind_y_cum\", \"SICobs_AMSR2_SIC\"]\n",
    "list_targets = [\"TARGET_AMSR2_SIC\"]\n",
    "#\n",
    "model_params = {\"list_predictors\": list_predictors,\n",
    "                \"list_targets\": list_targets, \n",
    "                \"patch_dim\": (480, 544),\n",
    "                \"batch_size\": 4,\n",
    "                \"n_filters\": [32, 64, 128, 256, 512, 1024],\n",
    "                \"activation\": \"relu\",\n",
    "                \"kernel_initializer\": \"he_normal\",\n",
    "                \"batch_norm\": True,\n",
    "                \"pooling_type\": \"Average\",\n",
    "                \"dropout\": 0,\n",
    "                }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Land-Sea mask\n",
    "\n",
    "    1: Ocean\n",
    "    0: Land"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_land_sea_mask(paths = paths, start_date = \"20180104\"):\n",
    "    filename = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc = netCDF4.Dataset(filename, \"r\")\n",
    "    LSM = nc.variables[\"LSM\"][:,:] \n",
    "    nc.close()\n",
    "    return(LSM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make_list_dates function\n",
    "\n",
    "    date_min: earliest date of the period (\"YYYYMMDD\")\n",
    "    date_max: latest date of the period (\"YYYYMMDD\")\n",
    "    frequency: \"daily\" or \"weekly\"\n",
    "    path_data: path where the data are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_list_dates(date_min, date_max, frequency, path_data, lead_time = lead_time):\n",
    "    current_date = datetime.datetime.strptime(date_min, '%Y%m%d')\n",
    "    end_date = datetime.datetime.strptime(date_max, '%Y%m%d')\n",
    "    list_dates = []\n",
    "    while current_date <= end_date:\n",
    "        date_str = current_date.strftime('%Y%m%d')\n",
    "        filename = path_data + date_str[0:4] + \"/\" + date_str[4:6] + \"/\" + \"Dataset_\" + date_str + \".nc\"\n",
    "        if os.path.isfile(filename):\n",
    "            nc = netCDF4.Dataset(filename, \"r\")\n",
    "            TARGET_AMSR2_SIC = nc.variables[\"TARGET_AMSR2_SIC\"][lead_time,:,:]\n",
    "            nc.close()\n",
    "            if np.sum(np.isnan(TARGET_AMSR2_SIC)) == 0:\n",
    "                list_dates.append(date_str)\n",
    "        #\n",
    "        if frequency == \"daily\":\n",
    "            current_date = current_date + datetime.timedelta(days = 1)\n",
    "        elif frequency == \"weekly\":\n",
    "            current_date = current_date + datetime.timedelta(days = 7)\n",
    "    return(list_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standardization_data(file_standardization):\n",
    "    standard = {}\n",
    "    hf = h5py.File(file_standardization, \"r\")\n",
    "    for var in hf:\n",
    "        if \"ECMWF\" in var:\n",
    "            standard[var] = np.array(hf[var])[lead_time]\n",
    "        else:\n",
    "            standard[var] = hf[var][()]\n",
    "    hf.close()\n",
    "    return(standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eval_data(start_date, lead_time):\n",
    "    previous_day = (datetime.datetime.strptime(start_date, \"%Y%m%d\") - datetime.timedelta(days = 1)).strftime(\"%Y%m%d\")\n",
    "    #\n",
    "    Eval_data = {}\n",
    "    filename_training = paths[\"training\"] + start_date[0:4] + \"/\" + start_date[4:6] + \"/\" + \"Dataset_\" + start_date + \".nc\"\n",
    "    nc_training = netCDF4.Dataset(filename_training, \"r\")\n",
    "    #\n",
    "    for var in [\"x\", \"y\", \"lat\", \"lon\", \"TARGET_AMSR2_SIC\", \"SICobs_AMSR2_SIC\"]:\n",
    "        if nc_training.variables[var].ndim == 1:\n",
    "            Eval_data[var] = nc_training.variables[var][:]\n",
    "        elif nc_training.variables[var].ndim == 2:\n",
    "            Eval_data[var] = nc_training.variables[var][:,:]\n",
    "        elif nc_training.variables[var].ndim == 3:\n",
    "            Eval_data[var] = nc_training.variables[var][lead_time,:,:]\n",
    "    nc_training.close()\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_10\"] = np.zeros(np.shape(Eval_data[\"TARGET_AMSR2_SIC\"]))\n",
    "    Eval_data[\"TARGET_AMSR2_SIE_10\"][Eval_data[\"TARGET_AMSR2_SIC\"] >= 10] = 1\n",
    "    #\n",
    "    target_date = (datetime.datetime.strptime(start_date, \"%Y%m%d\") + datetime.timedelta(days = lead_time)).strftime(\"%Y%m%d\")\n",
    "    file_ice_edge_lengths = paths[\"ice_edge_lengths\"] + target_date[0:4] + \"/\" + target_date[4:6] + \"/\" + \"Ice_edge_lengths_\" + target_date + \".h5\"\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Eval_data[var] = np.array(hf[var])\n",
    "    hf.close()    \n",
    "    #\n",
    "    return(Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions in netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_in_netCDF(start_date, Pred_data, Eval_data, paths = paths):\n",
    "    file_output = paths[\"predictions_netCDF\"] + \"AICE_forecasts_\" + start_date + \".nc\"\n",
    "    output_netcdf = netCDF4.Dataset(file_output, 'w', format = 'NETCDF4')\n",
    "    Outputs = vars()\n",
    "    #\n",
    "    dimensions = [\"x\", \"y\"]\n",
    "    for di in dimensions:\n",
    "        Outputs[di] = output_netcdf.createDimension(di, len(Eval_data[di]))\n",
    "    #\n",
    "    dim_variables = dimensions + [\"lat\", \"lon\"]\n",
    "    #\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"] = output_netcdf.createVariable(\"Lambert_Azimuthal_Grid\", \"d\")\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].grid_mapping_name = \"lambert_azimuthal_equal_area\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].semi_major_axis = 6378137\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].semi_minor_axis = 6356752.31424518\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].reference_ellipsoid_name = \"WGS 84\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].longitude_of_prime_meridian = \"0.0\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].prime_meridian_name = \"Greenwich\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].geographic_crs_name = \"unknown\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].horizontal_datum_name = \"Unknown based on WGS84 ellipsoid\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].projected_crs_name = \"unknown\"\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].latitude_of_projection_origin = 90.0\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].longitude_of_projection_origin = 0.0\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].false_easting = 0.0\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].false_northing = 0.0\n",
    "    Outputs[\"Lambert_Azimuthal_Grid\"].proj4_string = \"+ellps=WGS84 +lat_0=90 +lon_0=0 +no_defs=None +proj=laea +type=crs +units=m +x_0=0 +y_0=0\"\n",
    "    #\n",
    "    for dv in dim_variables:\n",
    "        if Eval_data[dv].ndim == 1:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (dv))\n",
    "            Outputs[dv][:] = Eval_data[dv]   \n",
    "            if dv == \"x\" or dv == \"y\":\n",
    "                Outputs[dv].standard_name = \"projection_\" + dv + \"_coordinate\"\n",
    "                Outputs[dv].units = \"m\"\n",
    "        elif Eval_data[dv].ndim == 2:\n",
    "            Outputs[dv] = output_netcdf.createVariable(dv, \"d\", (\"y\", \"x\"))\n",
    "            Outputs[dv][:,:] = Eval_data[dv]\n",
    "            if dv == \"lat\":\n",
    "                Outputs[dv].standard_name = \"latitude\"\n",
    "            elif dv == \"lon\":\n",
    "                Outputs[dv].standard_name = \"longitude\"\n",
    "            Outputs[dv].units = \"degrees\"\n",
    "    #\n",
    "    for var in Eval_data:\n",
    "        if \"Ice_edge_lengths_SIC\" in var:\n",
    "            pass\n",
    "        else:\n",
    "            if (var in dim_variables) == False:\n",
    "                Outputs[var] = output_netcdf.createVariable(var, \"d\", (\"y\", \"x\"))\n",
    "                Outputs[var][:,:] = Eval_data[var]\n",
    "                if \"SIC\" in var:\n",
    "                    Outputs[var].standard_name = \"sea ice concentration\"\n",
    "                    Outputs[var].units = \"%\"\n",
    "                elif \"SIE\" in var:\n",
    "                    Outputs[var].standard_name = \"sea ice extent\"\n",
    "                    Outputs[var].units = \"1 if sea ice concentration higher than \" + var[-2:len(var)] + \" %, 0 otherwise\"\n",
    "    #\n",
    "    for var in Pred_data:\n",
    "        Outputs[var] = output_netcdf.createVariable(var, \"d\", (\"y\", \"x\"))\n",
    "        Outputs[var][:,:] = Pred_data[var]\n",
    "        if \"SIC\" in var:\n",
    "            Outputs[var].standard_name = var\n",
    "            Outputs[var].units = \"%\" \n",
    "        elif \"SIE\" in var:\n",
    "            Outputs[var].standard_name = \"Predicted sea ice extent (SIC threshold \" + var[-2:len(var)] + \" %)\"\n",
    "            Outputs[var].units = \"1: ice, 0: ice-free\"   \n",
    "    #\n",
    "    output_netcdf.Conventions = \"CF-1.8, ACDD-1.3\"\n",
    "    output_netcdf.title = \"AICE sea ice concentration forecasts\"\n",
    "    output_netcdf.summary = \"Short-range sea ice concentration forecasts produced using deep learning at a spatial resolution of 5 km.\"\n",
    "    output_netcdf.keywords = \"Sea ice concentration forecasts, Deep learning, European Arctic\"\n",
    "    output_netcdf.area = \"European Arctic\"\n",
    "    output_netcdf.institution = \"Norwegian Meteorological Institute\"\n",
    "    output_netcdf.PI_name = \"Cyril Palerme\"\n",
    "    output_netcdf.contact = \"cyril.palerme@met.no\"\n",
    "    output_netcdf.bulletin_type = \"Forecast\"\n",
    "    output_netcdf.forecast_range = \"1 day\"\n",
    "    output_netcdf.time_coverage_start = start_date\n",
    "    output_netcdf.time_coverage_stop = start_date\n",
    "    #\n",
    "    output_netcdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for calculating SIC from predictions, and for making binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIC_from_normalized_SIC(variable_name, field, standard):\n",
    "    Predicted_SIC = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def SIC_from_normalized_model_error(variable_name, field, standard, si_model_SIC):\n",
    "    model_error = field * (standard[variable_name + \"_max\"] - standard[variable_name + \"_min\"]) + standard[variable_name + \"_min\"]\n",
    "    print(\"model_error\", np.min(model_error), np.max(model_error), np.mean(model_error), np.median(model_error))\n",
    "    Predicted_SIC = si_model_SIC - model_error\n",
    "    Predicted_SIC[Predicted_SIC > 100] = 100\n",
    "    Predicted_SIC[Predicted_SIC < 0] = 0\n",
    "    return(Predicted_SIC)\n",
    "#\n",
    "def binary_classification(field, threshold):\n",
    "    output = np.zeros(np.shape(field))\n",
    "    output[field > threshold] = 1\n",
    "    return(output)\n",
    "#\n",
    "def RMSE(SIC_forecasts, SIC_observations, LSM):\n",
    "    SIC_forecasts = np.ndarray.flatten(SIC_forecasts[LSM == 1])\n",
    "    SIC_observations = np.ndarray.flatten(SIC_observations[LSM == 1])\n",
    "    MSE = np.sum((SIC_forecasts - SIC_observations) ** 2) / len(SIC_observations)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    return(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ice_edge_lengths(file_ice_edge_lengths):\n",
    "    Dataset = {}\n",
    "    hf = h5py.File(file_ice_edge_lengths, \"r\")\n",
    "    for var in hf:\n",
    "        Dataset[var] = np.array(hf[var])\n",
    "    hf.close()\n",
    "    return(Dataset)\n",
    "#\n",
    "def IIEE(SIE_obs, SIE_forecast, grid_cell_area):\n",
    "    Flag_SIE = np.full(np.shape(SIE_obs), np.nan)\n",
    "    Flag_SIE[SIE_forecast == SIE_obs] = 0\n",
    "    Flag_SIE[SIE_forecast < SIE_obs] = -1\n",
    "    Flag_SIE[SIE_forecast > SIE_obs] = 1\n",
    "    Underestimation = np.sum(Flag_SIE == -1) * grid_cell_area\n",
    "    Overestimation = np.sum(Flag_SIE == 1) * grid_cell_area\n",
    "    IIEE_metric = Underestimation + Overestimation\n",
    "    return(IIEE_metric, Underestimation, Overestimation)\n",
    "#\n",
    "def SPS(SIP_obs, SIP_forecast, grid_cell_area):\n",
    "    SPS_metric = np.nansum(grid_cell_area * (SIP_forecast - SIP_obs)**2)\n",
    "    return(SPS_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area):\n",
    "    day_of_year = int(datetime.datetime.strptime(start_date, \"%Y%m%d\").strftime('%j'))\n",
    "    #\n",
    "    ML = {}\n",
    "    ML[\"SIE_10\"] = binary_classification(Pred_data[\"Predicted_SIC\"], 10)\n",
    "    #\n",
    "    ML_post_processed = {}\n",
    "    ML_post_processed[\"SIC\"] = np.copy(Pred_data[\"Predicted_SIC\"])\n",
    "    ML_post_processed[\"SIC\"][Pred_data[\"Predicted_SIC\"] < 3] = 0\n",
    "    #\n",
    "    Persistence = {}\n",
    "    Persistence[\"SIE_10\"] = binary_classification(Eval_data[\"SICobs_AMSR2_SIC\"], 10)\n",
    "    #\n",
    "    Metrics = {}\n",
    "    Metrics[\"start_date\"] = start_date\n",
    "    Metrics[\"Ice_edge_length_SIC10\"] = Eval_data[\"Ice_edge_lengths_SIC10\"]\n",
    "    #\n",
    "    Metrics[\"RMSE_ML\"] = RMSE(Pred_data[\"Predicted_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_ML_post_processed\"] = RMSE(ML_post_processed[\"SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    Metrics[\"RMSE_Persistence\"] = RMSE(Eval_data[\"SICobs_AMSR2_SIC\"], Eval_data[\"TARGET_AMSR2_SIC\"], LSM)\n",
    "    #\n",
    "    Metrics[\"IIEElength_10_ML\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], ML[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]\n",
    "    Metrics[\"IIEElength_10_Persistence\"] = IIEE(Eval_data[\"TARGET_AMSR2_SIE_10\"], Persistence[\"SIE_10\"], grid_cell_area)[0] / Metrics[\"Ice_edge_length_SIC10\"]  \n",
    "    #\n",
    "    for var in Metrics:\n",
    "        if var != \"start_date\":\n",
    "            if \"RMSE\" in var:\n",
    "                Metrics[var] = np.round(Metrics[var], 3)\n",
    "            else:\n",
    "                Metrics[var] = np.round(Metrics[var])\n",
    "    #\n",
    "    return(Metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write_scores function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_scores(Metrics, paths = paths):\n",
    "    header = \"\"\n",
    "    scores = \"\"\n",
    "    for var in Metrics:\n",
    "        header = header + \"\\t\" + var   \n",
    "        scores = scores + \"\\t\" + str(Metrics[var]) \n",
    "    #\n",
    "    output_file = paths[\"prediction_scores\"] + \"Scores_\" + date_min_test + \"_\" + date_max_test + \".txt\"\n",
    "    if start_date == date_min_test:\n",
    "        if os.path.isfile(output_file) == True:\n",
    "            os.system(\"rm \" + output_file)\n",
    "    #\n",
    "    if os.path.isfile(output_file) == False:\n",
    "        output = open(output_file, 'a')\n",
    "        output.write(header + \"\\n\")\n",
    "        output.close()\n",
    "    #\n",
    "    output = open(output_file, 'a')\n",
    "    output.write(scores + \"\\n\")\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make predictions functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_predictions(start_date, model, standard, LSM):\n",
    "    Eval_data = extract_eval_data(start_date, lead_time)\n",
    "    #\n",
    "    params_test = {\"list_predictors\": model_params[\"list_predictors\"],\n",
    "                    \"list_labels\": model_params[\"list_targets\"],\n",
    "                    \"list_dates\": [start_date],\n",
    "                    \"lead_time\": lead_time,\n",
    "                    \"standard\": standard,\n",
    "                    \"batch_size\": 1,\n",
    "                    \"path_data\": paths[\"training\"],\n",
    "                    \"dim\": model_params[\"patch_dim\"],\n",
    "                    \"shuffle\": False,\n",
    "                    }\n",
    "    #\n",
    "    test_generator = Data_generator(**params_test)\n",
    "    predictions = np.squeeze(model.predict(test_generator))\n",
    "    predictions = SIC_from_normalized_SIC(\"TARGET_AMSR2_SIC\", predictions, standard)\n",
    "    predictions[:,:][LSM == 0] = 0\n",
    "    # \n",
    "    Pred_data = {}\n",
    "    Pred_data[\"Predicted_SIC\"] = np.copy(predictions[:,:])\n",
    "    #\n",
    "    return(Pred_data, Eval_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = load_land_sea_mask()\n",
    "standard = load_standardization_data(file_standardization)\n",
    "list_dates_test = make_list_dates(date_min_test, date_max_test, frequency = \"daily\", path_data = paths[\"training\"])\n",
    "#\n",
    "unet_model = Att_Res_UNet(**model_params).make_unet_model()\n",
    "unet_model.load_weights(file_model_weights)\n",
    "#print(unet_model.summary())\n",
    "#\n",
    "Scores = {}\n",
    "for sd, start_date in enumerate(list_dates_test):\n",
    "    print(\"forecast start_date\", start_date)\n",
    "    #try:\n",
    "    Pred_data, Eval_data = make_predictions(start_date = start_date, model = unet_model, standard = standard, LSM = LSM)\n",
    "    save_predictions_in_netCDF(start_date, Pred_data, Eval_data)\n",
    "    Metrics = verification_scores(Pred_data, Eval_data, start_date, LSM, grid_cell_area = grid_cell_area)\n",
    "    save_scores(Metrics, paths = paths)\n",
    "    #except:\n",
    "    #    pass\n",
    "#\n",
    "t1 = time.time()\n",
    "dt = t1 - t0\n",
    "#\n",
    "print(\"Predictions made ! Time: \", dt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
